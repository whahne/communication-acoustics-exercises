{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[exercises](brir.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import loadmat\n",
    "import soundfile as sf\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brir_clap, clap_fs = sf.read('data/brir_clap.wav')\n",
    "clap_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brir_clap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brir_clap) / clap_fs  # duration in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_contents = loadmat('data/brir_sweep.mat', struct_as_record=False, squeeze_me=True)\n",
    "mat_contents['__header__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mat_contents['data']\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.SystemLatencySamples, data.SystemlatencyRemoved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_fs = data.fs\n",
    "assert sweep_fs == clap_fs\n",
    "sweep_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head_azimuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brir_sweep = data.ir\n",
    "brir_sweep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brir_sweep) / sweep_fs  # duration in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech, speech_fs = sf.read('data/xmas.wav')\n",
    "speech.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert speech_fs == clap_fs == sweep_fs\n",
    "fs = speech_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_multichannel_ir(x, h, **kwargs):\n",
    "    \"\"\"Convolve mono signal x with multichannel impulse reponse h.\n",
    "    \n",
    "    This is somewhat inefficient, because the FFT of x is repeated\n",
    "    multiple times.\n",
    "    \n",
    "    After the convolution, the result is normalized to the maximum\n",
    "    amplitude of x.    \n",
    "    \n",
    "    \"\"\"\n",
    "    x = np.squeeze(np.asarray(x))\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"x must be a mono signal\")\n",
    "        \n",
    "    h = np.asarray(h)\n",
    "    if h.ndim == 1:\n",
    "        h = h.reshape(-1, 1)\n",
    "        \n",
    "    y = np.column_stack([signal.fftconvolve(x, ir, **kwargs) for ir in h.T])\n",
    "    y = tools.normalize(y, np.max(np.abs(x)))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `scipy.signal.fftconvolve()` does n-dimensional convolution.\n",
    "Since we only need one-dimensional convolution, we have to call it with one-dimensional arrays.\n",
    "\n",
    "Probably, some time in the future, `scipy.signal.fftconvolve()` will grow an `axis` argument ...\n",
    "\n",
    "https://github.com/scipy/scipy/issues/3525\n",
    "\n",
    "http://docs.scipy.org/doc/numpy/reference/generated/numpy.apply_along_axis.html\n",
    "\n",
    "http://stackoverflow.com/a/29678671/500098\n",
    "\n",
    "http://nipy.org/nitime/api/generated/nitime.utils.html#nitime.utils.fftconvolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_clap = convolve_multichannel_ir(speech, brir_clap)\n",
    "speech_clap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_sweep = convolve_multichannel_ir(speech, brir_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write('data/xmas_brir_clap.wav', speech_clap, fs)\n",
    "sf.write('data/xmas_brir_sweep.wav', speech_sweep, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio src=\"data/xmas.wav\" controls></audio>\n",
    "[data/xmas.wav](data/xmas.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_clap.wav\" controls></audio>\n",
    "[data/xmas_brir_clap.wav](data/xmas_brir_clap.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_sweep.wav\" controls></audio>\n",
    "[data/xmas_brir_sweep.wav](data/xmas_brir_sweep.wav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcomp, hcomp_fs = sf.read('data/THOMSON_HED415N_KEMAR_hcomp.wav')\n",
    "assert hcomp_fs == fs\n",
    "hcomp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compensate(brir):\n",
    "    \"\"\"Apply headphone compensation to a BRIR.\n",
    "\n",
    "    Side note: The variable \"hcomp\" is taken from the global scope\n",
    "    (it is *not* a function argument)!\n",
    "\n",
    "    \"\"\"\n",
    "    return np.column_stack([signal.fftconvolve(in1, in2)\n",
    "                            for in1, in2 in zip(brir.T, hcomp.T)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brir_clap_hcomp = compensate(brir_clap)\n",
    "brir_sweep_hcomp = compensate(brir_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_clap_hcomp = convolve_multichannel_ir(speech, brir_clap_hcomp)\n",
    "speech_sweep_hcomp = convolve_multichannel_ir(speech, brir_sweep_hcomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write('data/xmas_brir_clap_hcomp.wav', speech_clap_hcomp, fs)\n",
    "sf.write('data/xmas_brir_sweep_hcomp.wav', speech_sweep_hcomp, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio src=\"data/xmas.wav\" controls></audio>\n",
    "[data/xmas.wav](data/xmas.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_clap.wav\" controls></audio>\n",
    "[data/xmas_brir_clap.wav](data/xmas_brir_clap.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_clap_hcomp.wav\" controls></audio>\n",
    "[data/xmas_brir_clap_hcomp.wav](data/xmas_brir_clap_hcomp.wav)\n",
    "\n",
    "---\n",
    "\n",
    "<audio src=\"data/xmas.wav\" controls></audio>\n",
    "[data/xmas.wav](data/xmas.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_sweep.wav\" controls></audio>\n",
    "[data/xmas_brir_sweep.wav](data/xmas_brir_sweep.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_sweep_hcomp.wav\" controls></audio>\n",
    "[data/xmas_brir_sweep_hcomp.wav](data/xmas_brir_sweep_hcomp.wav)\n",
    "\n",
    "---\n",
    "\n",
    "Note that the headphone compensation filter used here was designed for THOMSON HED415N headphones.\n",
    "If you use any other type of headphones, the result might not sound better, it might even sound worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot BRIRs\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(tools.db(tools.normalize(brir_clap)))\n",
    "plt.title('BRIR clap')\n",
    "plt.ylim(-200,0)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(tools.db(tools.normalize(brir_clap_hcomp)))\n",
    "plt.title('BRIR clap headphone-compensated')\n",
    "plt.ylim(-200,0)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(tools.db(tools.normalize(brir_sweep)))\n",
    "plt.title('BRIR sweep')\n",
    "plt.ylim(-200,0)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(tools.db(tools.normalize(brir_sweep_hcomp)))\n",
    "plt.title('BRIR sweep headphone-compensated')\n",
    "plt.ylim(-200,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate time-of-flight\n",
    "np.argmax(brir_clap)/clap_fs * 343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate SNR\n",
    "print(tools.db(np.var(brir_clap[40000:]), power = True))\n",
    "print(tools.db(np.var(brir_clap_hcomp[40000:]), power = True))\n",
    "print(tools.db(np.var(brir_sweep[50000:]), power = True))\n",
    "print(tools.db(np.var(brir_sweep_hcomp[50000:]), power = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot magnitude spectrum\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(tools.db(tools.normalize(np.fft.rfft(brir_clap[:,0]))))\n",
    "plt.plot(tools.db(tools.normalize(np.fft.rfft(brir_clap[:,1]))))\n",
    "plt.xscale('log')\n",
    "plt.title('BRIR clap');\n",
    "plt.xlabel('f in Hz');\n",
    "plt.ylabel('magnitude in dB');\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(tools.db(tools.normalize(np.fft.rfft(brir_sweep[:,0]))))\n",
    "plt.plot(tools.db(tools.normalize(np.fft.rfft(brir_sweep[:,1]))))\n",
    "plt.xscale('log')\n",
    "plt.title('BRIR sweep');\n",
    "plt.xlabel('f in Hz');\n",
    "plt.ylabel('magnitude in dB');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ir_from_mat(angle):\n",
    "    filename = 'data/brir_sweep{:+03d}.mat'.format(angle)\n",
    "    data = loadmat(filename, struct_as_record=False, squeeze_me=True)['data']\n",
    "    assert data.fs == fs\n",
    "    assert np.isclose(np.rad2deg(data.head_azimuth), angle)\n",
    "    return data.ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = -80, -40, 40, 80\n",
    "brirs = {angle: load_ir_from_mat(angle) for angle in angles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brirs_hcomp = {angle: compensate(ir) for angle, ir in brirs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_hcomp = {angle: convolve_multichannel_ir(speech, ir)\n",
    "                for angle, ir in brirs_hcomp.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio src=\"data/xmas.wav\" controls></audio>\n",
    "[data/xmas.wav](data/xmas.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_sweep_hcomp-80.wav\" controls></audio>\n",
    "[data/xmas_brir_sweep_hcomp-80.wav](data/xmas_brir_sweep_hcomp-80.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_sweep_hcomp-40.wav\" controls></audio>\n",
    "[data/xmas_brir_sweep_hcomp-40.wav](data/xmas_brir_sweep_hcomp-40.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_sweep_hcomp.wav\" controls></audio>\n",
    "[data/xmas_brir_sweep_hcomp.wav](data/xmas_brir_sweep_hcomp.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_sweep_hcomp+40.wav\" controls></audio>\n",
    "[data/xmas_brir_sweep_hcomp+40.wav](data/xmas_brir_sweep_hcomp+40.wav)\n",
    "\n",
    "<audio src=\"data/xmas_brir_sweep_hcomp+80.wav\" controls></audio>\n",
    "[data/xmas_brir_sweep_hcomp+80.wav](data/xmas_brir_sweep_hcomp+80.wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot impulse responses of 1 ear for all angles\n",
    "\n",
    "time = np.arange(len(brir_sweep)) / sweep_fs\n",
    "\n",
    "for angle in angles:\n",
    "    plt.plot(time, brirs[angle][:,0])\n",
    "    \n",
    "plt.xlabel('time in s');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot magnitude spectra\n",
    "\n",
    "freq = np.arange(len(data.ir)/2 +1) * data.fs/(len(data.ir))\n",
    "\n",
    "for angle in angles:\n",
    "    plt.plot(freq, np.abs(np.fft.rfft(brirs[angle][:,0])), label = '%i' %angle)\n",
    "\n",
    "plt.legend();\n",
    "plt.xlabel('frequency in Hz');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the top right is the angle of the loudspeaker, relative to the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url =  'https://dev.qu.tu-berlin.de/projects/measurements/repository/raw/2010-11-kemar-anechoic/mat/QU_KEMAR_anechoic_2m.mat'\n",
    "file = io.BytesIO(urlopen(url).read())\n",
    "\n",
    "# if you want to load a local file instead:\n",
    "#file = \"QU_KEMAR_anechoic_2m.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs = loadmat(file, struct_as_record=False, squeeze_me=True)['irs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(irs.left);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(irs.left, aspect='auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but still: what?\n",
    "\n",
    "Let's try to convert the values to decibels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(20 * np.log10(np.abs(irs.left)), aspect='auto')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we're getting somewhere, but there are still things to do:\n",
    "\n",
    "* replace x- and y-indices with meaningful values\n",
    "\n",
    "* axis labels\n",
    "\n",
    "* better colormap (the default colormap 'jet' is considered harmful, see e.g. https://jakevdp.github.io/blog/2014/10/16/how-bad-is-your-colormap/)\n",
    "\n",
    "  * try those: 'YlGnBu', 'YlGn', 'RdPu', 'PuRd', 'PuBuGn', 'Greys'\n",
    "  \n",
    "  * if you have matplotlib >= 1.5, you should also try 'viridis', which will become the new standard color map in matplotlib 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_irs(samples):\n",
    "    extent = [np.rad2deg(irs.apparent_azimuth[0]),\n",
    "              np.rad2deg(irs.apparent_azimuth[-1]),\n",
    "              len(irs.left[:samples]) * 1000 / irs.fs,\n",
    "              0]\n",
    "    plt.imshow(20 * np.log10(np.abs(irs.left[:samples])), aspect='auto',\n",
    "               vmax=0, vmin=-120,\n",
    "               cmap='YlGnBu', extent=extent)\n",
    "    plt.xlabel(\"apparent source azimuth / degrees\")\n",
    "    plt.ylabel(\"time / milliseconds\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "plot_irs(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_irs(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_irs(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: waterfall diagrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p xmlns:dct=\"http://purl.org/dc/terms/\">\n",
    "  <a rel=\"license\"\n",
    "     href=\"http://creativecommons.org/publicdomain/zero/1.0/\">\n",
    "    <img src=\"http://i.creativecommons.org/p/zero/1.0/88x31.png\" style=\"border-style: none;\" alt=\"CC0\" />\n",
    "  </a>\n",
    "  <br />\n",
    "  To the extent possible under law,\n",
    "  <span rel=\"dct:publisher\" resource=\"[_:publisher]\">the person who associated CC0</span>\n",
    "  with this work has waived all copyright and related or neighboring\n",
    "  rights to this work.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
